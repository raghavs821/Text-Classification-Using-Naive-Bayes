{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re, string\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes=[]\n",
    "for root,dirc,file in os.walk(\"20_newsgroups\"):\n",
    "    for d in dirc:\n",
    "        classes.append(d)\n",
    "classes\n",
    "# this contain all the 20 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root addres of each subfolder\n",
    "roots = [root for root,dirc,file in os.walk(\"20_newsgroups\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for root,dirc,file in os.walk(\"20_newsgroups\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files.pop(0)# for removing the first element from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the dictionary containing the word and the frequency\n",
    "def Build_dictionary(words):\n",
    "    for w in words:\n",
    "        if w not in vocab.keys():\n",
    "            vocab[w] = 1\n",
    "        else:\n",
    "            vocab[w] += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for removing the stopwords\n",
    "def clean_data(words):\n",
    "    output = []\n",
    "    for w in words:\n",
    "        if w not in stops:\n",
    "            output.append(w)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9044887\n"
     ]
    }
   ],
   "source": [
    "# for reading the each document and breaking them into  words\n",
    "k = 0\n",
    "y = []\n",
    "words_data=[]\n",
    "clean_words_data=[]\n",
    "for f in files:\n",
    "    k=k+1\n",
    "    for i in f:\n",
    "        y.append(classes[k-1])\n",
    "        file_object=open(roots[k] + \"/\" + i,\"r\")\n",
    "        file_data = file_object.read()\n",
    "        words_data = re.split(\"[\" + string.punctuation + \" \" + \"]\", file_data.lower())\n",
    "        clean_words_data += clean_data(words_data)\n",
    "print(len(clean_words_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "vocab = Build_dictionary(clean_words_data)\n",
    "#print(vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469\n"
     ]
    }
   ],
   "source": [
    "# finding how many features to be taken\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "x1 = []\n",
    "#x1 = x1.sort(reverse=True)\n",
    "x1 = ([vocab[w] for w in vocab.keys()])\n",
    "x1.sort(reverse=True)\n",
    "x1 = np.array(x1)\n",
    "x2 = np.arange(0,len(vocab.keys()),1)\n",
    "print(x1[1500])\n",
    "#so we are going to use 1500 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list contain the features sorted by the frequencies\n",
    "l = []\n",
    "for w in sorted(vocab, key=vocab.get, reverse=True):\n",
    "    l.append((w, vocab[w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set=[l[i][0] for i in range(0,1500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'edu',\n",
       " '\\n',\n",
       " 'ax',\n",
       " 'cmu',\n",
       " 'cs',\n",
       " 'com',\n",
       " '\\n\\n',\n",
       " 'news',\n",
       " 'srv',\n",
       " 'cantaloupe',\n",
       " 'net',\n",
       " '1',\n",
       " 'id',\n",
       " 'apr',\n",
       " 'state',\n",
       " '1993',\n",
       " 'ohio',\n",
       " 'writes',\n",
       " 'would',\n",
       " '2',\n",
       " 'one',\n",
       " '\\ndate',\n",
       " 'comp',\n",
       " '0',\n",
       " '3',\n",
       " 'article',\n",
       " 'talk',\n",
       " 'ans',\n",
       " 'howland',\n",
       " 'reston',\n",
       " '\\nsubject',\n",
       " 'posting',\n",
       " 'ca',\n",
       " 'like',\n",
       " 'misc',\n",
       " 'people',\n",
       " 'mps',\n",
       " 'cc',\n",
       " 'host',\n",
       " '93',\n",
       " '\\nnewsgroups',\n",
       " 'know',\n",
       " 'sci',\n",
       " '5',\n",
       " 'zaphod',\n",
       " '4',\n",
       " 'get',\n",
       " 'alt',\n",
       " 'usenet',\n",
       " '\\\\',\n",
       " 'politics',\n",
       " 'university',\n",
       " 'think',\n",
       " 'may',\n",
       " 'path',\n",
       " 'c',\n",
       " 'gmt\\nlines',\n",
       " 'windows',\n",
       " 'rutgers',\n",
       " 'x',\n",
       " 'newsgroups',\n",
       " 'harvard',\n",
       " '6',\n",
       " 'eng',\n",
       " 'near',\n",
       " '\\nsender',\n",
       " 'also',\n",
       " '\\norganization',\n",
       " '\\nmessage',\n",
       " 'time',\n",
       " 'w',\n",
       " 'rec',\n",
       " '7',\n",
       " 'use',\n",
       " 'r',\n",
       " 'crabapple',\n",
       " 'sei',\n",
       " 'noc',\n",
       " 'e',\n",
       " 'soc',\n",
       " 'p',\n",
       " 'u',\n",
       " 'new',\n",
       " 'xref',\n",
       " 'das',\n",
       " '\\n\\t',\n",
       " '20',\n",
       " 'q',\n",
       " '16',\n",
       " 'us',\n",
       " 'cis',\n",
       " 'good',\n",
       " 'the\\n',\n",
       " 'system',\n",
       " 'g',\n",
       " '\\nnntp',\n",
       " '8',\n",
       " 'europa',\n",
       " 'gtefsd',\n",
       " 'could',\n",
       " 'ece',\n",
       " '15',\n",
       " 'uunet',\n",
       " 'religion',\n",
       " 'fs7',\n",
       " 'well',\n",
       " 'even',\n",
       " 'see',\n",
       " '\\n\\ni',\n",
       " 'way',\n",
       " 'b',\n",
       " '21',\n",
       " 'god',\n",
       " 'say',\n",
       " 'rochester',\n",
       " 'two',\n",
       " 'make',\n",
       " 'much',\n",
       " 'org',\n",
       " '10',\n",
       " 'club',\n",
       " 'many',\n",
       " 'uk',\n",
       " 'hp',\n",
       " 'right',\n",
       " '\\nmax',\n",
       " 'ac',\n",
       " 'first',\n",
       " 'n',\n",
       " 'gov',\n",
       " 'gatech',\n",
       " '9',\n",
       " 'magnesium',\n",
       " '\\nreferences',\n",
       " 'udel',\n",
       " 'andrew',\n",
       " 'utexas',\n",
       " '23',\n",
       " '17',\n",
       " 'culture',\n",
       " 'ibm',\n",
       " 'de',\n",
       " 'sun',\n",
       " 'want',\n",
       " '19',\n",
       " 'sys',\n",
       " 'v',\n",
       " 'said',\n",
       " '22',\n",
       " 'uiuc',\n",
       " 'anyone',\n",
       " 'l',\n",
       " 'need',\n",
       " 'used',\n",
       " '18',\n",
       " 'nasa',\n",
       " '14',\n",
       " 'go',\n",
       " 'h',\n",
       " 'sura',\n",
       " 'work',\n",
       " 'j',\n",
       " 'mail',\n",
       " 'really',\n",
       " 'space',\n",
       " 'netcom',\n",
       " '\\n\\n\\n',\n",
       " 'k',\n",
       " 'problem',\n",
       " 'ms',\n",
       " 'os',\n",
       " 'computer',\n",
       " '00',\n",
       " 'believe',\n",
       " '\\nm',\n",
       " '12',\n",
       " 'something',\n",
       " 'mac',\n",
       " 'world',\n",
       " 'back',\n",
       " 'pc',\n",
       " 'still',\n",
       " 'f',\n",
       " 'years',\n",
       " 'edu\\norganization',\n",
       " 'since',\n",
       " 'z',\n",
       " 'point',\n",
       " '11',\n",
       " 'going',\n",
       " '13',\n",
       " 'find',\n",
       " '\\nthe',\n",
       " '\\nlines',\n",
       " 'help',\n",
       " 'take',\n",
       " 'mit',\n",
       " 'year',\n",
       " 'cso',\n",
       " 'might',\n",
       " 'better',\n",
       " 'christian',\n",
       " 'information',\n",
       " 'government',\n",
       " 'usc',\n",
       " '\\n\\nthe',\n",
       " 'never',\n",
       " 'read',\n",
       " 'please',\n",
       " 'question',\n",
       " 'etc',\n",
       " 'using',\n",
       " 'last',\n",
       " 'things',\n",
       " '24',\n",
       " 'gmt\\norganization',\n",
       " 'must',\n",
       " 'cwru',\n",
       " 'uucp',\n",
       " 'access',\n",
       " 'au',\n",
       " 'made',\n",
       " 'software',\n",
       " 'file',\n",
       " '25',\n",
       " 'sure',\n",
       " 'number',\n",
       " 'case',\n",
       " 'got',\n",
       " '\\t',\n",
       " 'without',\n",
       " '30',\n",
       " 'graphics',\n",
       " 'data',\n",
       " 'someone',\n",
       " 'thing',\n",
       " 'look',\n",
       " 'co',\n",
       " 'fact',\n",
       " 'apple',\n",
       " 'bit',\n",
       " 'long',\n",
       " 'another',\n",
       " 'part',\n",
       " 'columbia',\n",
       " 'bb3',\n",
       " 'program',\n",
       " 'darwin',\n",
       " 'tue',\n",
       " 'version',\n",
       " 'law',\n",
       " 'come',\n",
       " 'day',\n",
       " 'of\\n',\n",
       " 'and\\n',\n",
       " 'drive',\n",
       " 'gmt\\nreferences',\n",
       " 'sport',\n",
       " 'fri',\n",
       " '\\ni',\n",
       " 'available',\n",
       " 'let',\n",
       " 'david',\n",
       " 'to\\n',\n",
       " '26',\n",
       " 'anything',\n",
       " 'power',\n",
       " '02',\n",
       " 'around',\n",
       " 'little',\n",
       " 'att',\n",
       " 'give',\n",
       " 'science',\n",
       " 'best',\n",
       " 'public',\n",
       " 'true',\n",
       " '1993apr20',\n",
       " 'key',\n",
       " 'seems',\n",
       " '\\nx',\n",
       " 'set',\n",
       " 'life',\n",
       " 'car',\n",
       " 'however',\n",
       " 'game',\n",
       " 'john',\n",
       " 'tell',\n",
       " 'course',\n",
       " 'put',\n",
       " 'line',\n",
       " '27',\n",
       " '04',\n",
       " 'least',\n",
       " 'real',\n",
       " 'every',\n",
       " 'inc',\n",
       " 'lot',\n",
       " '01',\n",
       " '50',\n",
       " 'run',\n",
       " 'research',\n",
       " 'enough',\n",
       " 'probably',\n",
       " '03',\n",
       " 'try',\n",
       " '\\nand',\n",
       " 'different',\n",
       " 'great',\n",
       " 'says',\n",
       " 'high',\n",
       " 'forsale',\n",
       " 'washington',\n",
       " 'post',\n",
       " 'far',\n",
       " 'man',\n",
       " 'name',\n",
       " 'dos',\n",
       " 'free',\n",
       " 'pitt',\n",
       " 'list',\n",
       " 'mean',\n",
       " 'hard',\n",
       " 'mon',\n",
       " 'stanford',\n",
       " 'support',\n",
       " 'ins',\n",
       " 'athos',\n",
       " 'thu',\n",
       " 'uwm',\n",
       " 'group',\n",
       " 'acs',\n",
       " 'though',\n",
       " '05',\n",
       " 'jesus',\n",
       " 'next',\n",
       " 'either',\n",
       " 'old',\n",
       " 'image',\n",
       " 'nothing',\n",
       " 'else',\n",
       " 'end',\n",
       " 'non',\n",
       " 'systems',\n",
       " 'agate',\n",
       " 'wrong',\n",
       " 'possible',\n",
       " 'called',\n",
       " 'umd',\n",
       " 'message',\n",
       " '1993apr15',\n",
       " 'second',\n",
       " 'actually',\n",
       " '1993apr21',\n",
       " 'call',\n",
       " 'card',\n",
       " 'reason',\n",
       " 'gun',\n",
       " 'purdue',\n",
       " '28',\n",
       " 'rather',\n",
       " 'unix',\n",
       " 'edu\\n',\n",
       " 'guns',\n",
       " 'looking',\n",
       " 'bill',\n",
       " 'order',\n",
       " 'a\\n',\n",
       " '40',\n",
       " 'others',\n",
       " 'gmt\\nsender',\n",
       " 'person',\n",
       " 'security',\n",
       " 'done',\n",
       " 'mr',\n",
       " 'able',\n",
       " 'based',\n",
       " 'seen',\n",
       " 'found',\n",
       " 'network',\n",
       " 'bad',\n",
       " 'atheism',\n",
       " 'turkish',\n",
       " 'example',\n",
       " 'wrote',\n",
       " '06',\n",
       " 'place',\n",
       " 'general',\n",
       " '07',\n",
       " 'ever',\n",
       " 'team',\n",
       " '09',\n",
       " 'keep',\n",
       " 'yet',\n",
       " 'wed',\n",
       " '\\ndistribution',\n",
       " 'always',\n",
       " 'info',\n",
       " 'colorado',\n",
       " 'problems',\n",
       " 'heard',\n",
       " 'following',\n",
       " 'files',\n",
       " 'geneva',\n",
       " 'human',\n",
       " 'sgi',\n",
       " '32',\n",
       " 'three',\n",
       " 'israel',\n",
       " 'jews',\n",
       " 'maybe',\n",
       " 'quite',\n",
       " 'send',\n",
       " '29',\n",
       " 'opinions',\n",
       " 'idea',\n",
       " '000',\n",
       " 'hardware',\n",
       " 'thought',\n",
       " 'book',\n",
       " 'means',\n",
       " 'left',\n",
       " 'chip',\n",
       " 'american',\n",
       " '08',\n",
       " 'jpl',\n",
       " 'trying',\n",
       " 'evidence',\n",
       " 'internet',\n",
       " 'control',\n",
       " 'away',\n",
       " 'email',\n",
       " 'less',\n",
       " 'berkeley',\n",
       " 'start',\n",
       " 'window',\n",
       " 'wupost',\n",
       " 'kind',\n",
       " 'virginia',\n",
       " 'whether',\n",
       " 'big',\n",
       " 'rights',\n",
       " 'jewish',\n",
       " 'children',\n",
       " 'times',\n",
       " 'word',\n",
       " 'today',\n",
       " 'caltech',\n",
       " '\\nreply',\n",
       " 'home',\n",
       " 'given',\n",
       " 'seem',\n",
       " 'yes',\n",
       " 'mark',\n",
       " 'phone',\n",
       " 'bogus',\n",
       " 'president',\n",
       " 'digex',\n",
       " 'national',\n",
       " '1993apr16',\n",
       " 'is\\n',\n",
       " 'sdd',\n",
       " 'getting',\n",
       " 'usa',\n",
       " 'ucs',\n",
       " 'makes',\n",
       " 'ftp',\n",
       " 'men',\n",
       " '34',\n",
       " 'several',\n",
       " 'change',\n",
       " 'service',\n",
       " 'win',\n",
       " 'thanks',\n",
       " 'toronto',\n",
       " 'michael',\n",
       " 'indiana',\n",
       " 'code',\n",
       " 'history',\n",
       " 'standard',\n",
       " 'fbi',\n",
       " 'magnus',\n",
       " 'that\\n',\n",
       " 'games',\n",
       " 'remember',\n",
       " 'questions',\n",
       " 'saying',\n",
       " 'scsi',\n",
       " 'server',\n",
       " 'open',\n",
       " '31',\n",
       " 'ask',\n",
       " 'center',\n",
       " 'technology',\n",
       " 'abortion',\n",
       " 'local',\n",
       " '1993apr19',\n",
       " 'full',\n",
       " 'money',\n",
       " '33',\n",
       " 'war',\n",
       " 'cannot',\n",
       " 'ago',\n",
       " 'fi',\n",
       " 'paul',\n",
       " 'stratus',\n",
       " '45',\n",
       " '44',\n",
       " 'price',\n",
       " 'answer',\n",
       " 'stuff',\n",
       " 'already',\n",
       " 'clipper',\n",
       " '1993apr22',\n",
       " 'large',\n",
       " 'wanted',\n",
       " 'small',\n",
       " 'matter',\n",
       " 'issue',\n",
       " 'buy',\n",
       " 'interested',\n",
       " 'disk',\n",
       " 'came',\n",
       " 'agree',\n",
       " 'note',\n",
       " 'play',\n",
       " 'bible',\n",
       " 'care',\n",
       " 'speed',\n",
       " 'days',\n",
       " 'box',\n",
       " 'in\\n',\n",
       " '41',\n",
       " 'running',\n",
       " 'show',\n",
       " 'g9v',\n",
       " 'source',\n",
       " 'told',\n",
       " 'address',\n",
       " 'clinton',\n",
       " 'reply',\n",
       " 'whole',\n",
       " 'sol',\n",
       " 'works',\n",
       " 'math',\n",
       " 'misc\\npath',\n",
       " 'fire',\n",
       " 'se',\n",
       " 'color',\n",
       " 'fax',\n",
       " 'earth',\n",
       " 'hand',\n",
       " 'claim',\n",
       " '35',\n",
       " '100',\n",
       " 'caen',\n",
       " 'b8f',\n",
       " 'states',\n",
       " 'pretty',\n",
       " 'mideast',\n",
       " 'netnews',\n",
       " 'perhaps',\n",
       " 'live',\n",
       " 'video',\n",
       " 'institute',\n",
       " 'user',\n",
       " 'cornell',\n",
       " '38',\n",
       " 'type',\n",
       " '1993apr23',\n",
       " 'frank',\n",
       " 'mind',\n",
       " '\\n\\nif',\n",
       " 'feel',\n",
       " 'jim',\n",
       " 'com\\n',\n",
       " 'bnr',\n",
       " 'original',\n",
       " 'world\\nmessage',\n",
       " 'understand',\n",
       " 'machine',\n",
       " '42',\n",
       " '37',\n",
       " 'armenian',\n",
       " 'current',\n",
       " 'misc\\nsubject',\n",
       " 'mike',\n",
       " 'everyone',\n",
       " 'edu\\n\\nin',\n",
       " 'black',\n",
       " 'buffalo',\n",
       " '36',\n",
       " 'igor',\n",
       " 'april',\n",
       " 'newsreader',\n",
       " 'love',\n",
       " 'com\\norganization',\n",
       " 'programs',\n",
       " '55',\n",
       " 'went',\n",
       " 'hope',\n",
       " '39',\n",
       " 'eff',\n",
       " 'subject',\n",
       " 'truth',\n",
       " 'area',\n",
       " 'ctr',\n",
       " 'robert',\n",
       " 'side',\n",
       " 'pay',\n",
       " 'cb',\n",
       " 'church',\n",
       " 'house',\n",
       " 'soviet',\n",
       " 'vs',\n",
       " 'guess',\n",
       " 'include',\n",
       " 'white',\n",
       " 'feed',\n",
       " 'important',\n",
       " '49',\n",
       " 'often',\n",
       " 'almost',\n",
       " '48',\n",
       " '59',\n",
       " 'everything',\n",
       " 'steve',\n",
       " 'simply',\n",
       " 'oz',\n",
       " 'started',\n",
       " 'mil',\n",
       " '56',\n",
       " 'health',\n",
       " 'value',\n",
       " 'sat',\n",
       " 'pacific',\n",
       " '51',\n",
       " 'cause',\n",
       " 'sandvik',\n",
       " 'legal',\n",
       " 'memory',\n",
       " 'christ',\n",
       " 'text',\n",
       " 'comes',\n",
       " 'autos',\n",
       " 'making',\n",
       " 'wisc',\n",
       " 'department',\n",
       " 'instead',\n",
       " 'gmt\\nmessage',\n",
       " 'crypt',\n",
       " 'display',\n",
       " '43',\n",
       " 'including',\n",
       " 'ucsd',\n",
       " 'ecn',\n",
       " 'working',\n",
       " 'death',\n",
       " 'dept',\n",
       " '54',\n",
       " 'hedrick',\n",
       " 'aramis',\n",
       " 'iastate',\n",
       " 'freenet',\n",
       " 'certainly',\n",
       " 'country',\n",
       " 'faq',\n",
       " 'christian\\nfrom',\n",
       " 'christian\\nsubject',\n",
       " '\\n\\nin',\n",
       " '145',\n",
       " 'school',\n",
       " 'cost',\n",
       " '\\nit',\n",
       " '\\nfrom',\n",
       " 'write',\n",
       " 'words',\n",
       " 'copy',\n",
       " 'view',\n",
       " 'light',\n",
       " 'dec',\n",
       " 'a86',\n",
       " 'hell',\n",
       " 'st',\n",
       " 'faith',\n",
       " 'single',\n",
       " 'opinion',\n",
       " '57',\n",
       " 'check',\n",
       " '\\n\\nthanks',\n",
       " 'cleveland',\n",
       " 'tried',\n",
       " 'computers',\n",
       " 'sort',\n",
       " 'low',\n",
       " 'uni',\n",
       " 'sale',\n",
       " 'ap',\n",
       " 'known',\n",
       " 'armenians',\n",
       " 'emory',\n",
       " 'mot',\n",
       " 'tin',\n",
       " 'encryption',\n",
       " '\\nbut',\n",
       " '46',\n",
       " 'clear',\n",
       " '52',\n",
       " 'later',\n",
       " 'sense',\n",
       " 'difference',\n",
       " 'deal',\n",
       " 'major',\n",
       " 'company',\n",
       " 'police',\n",
       " 'q\\t',\n",
       " 'although',\n",
       " 'top',\n",
       " 'christians',\n",
       " 'blue',\n",
       " 'college',\n",
       " 'objective',\n",
       " 'private',\n",
       " 'night',\n",
       " '58',\n",
       " '\\n\\nthis',\n",
       " '\\n\\nit',\n",
       " 'med',\n",
       " 'sni',\n",
       " 'consider',\n",
       " '\\nfollowup',\n",
       " 'morality',\n",
       " 'talking',\n",
       " 'city',\n",
       " 'society',\n",
       " '53',\n",
       " 'anybody',\n",
       " 'anyway',\n",
       " 'unless',\n",
       " 'mchp',\n",
       " 'hardware\\npath',\n",
       " 'saw',\n",
       " 'body',\n",
       " 'couple',\n",
       " '47',\n",
       " 'privacy',\n",
       " 'engineering',\n",
       " 'press',\n",
       " 'within',\n",
       " 'stop',\n",
       " 'happened',\n",
       " 'dead',\n",
       " 'koresh',\n",
       " 'elroy',\n",
       " 'self',\n",
       " 'force',\n",
       " 'argument',\n",
       " 'hardware\\nsubject',\n",
       " 'likely',\n",
       " 'peachnet',\n",
       " 'umn',\n",
       " 'package',\n",
       " 'asked',\n",
       " 'similar',\n",
       " 'swrinde',\n",
       " 'radio',\n",
       " 'austin',\n",
       " 'nice',\n",
       " 'religious',\n",
       " 'took',\n",
       " 'correct',\n",
       " 'tu',\n",
       " 'du',\n",
       " 'california',\n",
       " 'via',\n",
       " 'ed',\n",
       " 'fan',\n",
       " 'simple',\n",
       " 'hockey',\n",
       " 'theory',\n",
       " 'head',\n",
       " 'pl',\n",
       " 'sex',\n",
       " 'east',\n",
       " 'level',\n",
       " 'sound',\n",
       " '\\nin',\n",
       " 'san',\n",
       " 'ksu',\n",
       " 'size',\n",
       " 'books',\n",
       " 'women',\n",
       " 'moral',\n",
       " 'for\\n',\n",
       " 'fast',\n",
       " 'ok',\n",
       " 'especially',\n",
       " 'taken',\n",
       " 'political',\n",
       " 'per',\n",
       " 'cup',\n",
       " 'oh',\n",
       " 'israeli',\n",
       " 'form',\n",
       " 'board',\n",
       " 'goes',\n",
       " 'fine',\n",
       " 'isc',\n",
       " 'york',\n",
       " 'situation',\n",
       " 'services',\n",
       " 'ai',\n",
       " 'pub',\n",
       " 'provide',\n",
       " 'killed',\n",
       " 'mine',\n",
       " 'red',\n",
       " 'transfer',\n",
       " 'western',\n",
       " 'certain',\n",
       " 'corp',\n",
       " 'ames',\n",
       " 'upenn',\n",
       " 'statement',\n",
       " 'rest',\n",
       " 'screen',\n",
       " 'driver',\n",
       " 'written',\n",
       " 'format',\n",
       " 'business',\n",
       " 'yale',\n",
       " 'ogicse',\n",
       " 'groups',\n",
       " 'exactly',\n",
       " 'account',\n",
       " 'gmt\\narticle',\n",
       " 'james',\n",
       " 'common',\n",
       " '80',\n",
       " 'arizona',\n",
       " 'guy',\n",
       " 'laws',\n",
       " 'nl',\n",
       " 'become',\n",
       " 'early',\n",
       " 'bu',\n",
       " '\\nkeywords',\n",
       " '\\nto',\n",
       " 'whatever',\n",
       " 'position',\n",
       " 'past',\n",
       " 'upon',\n",
       " 'among',\n",
       " 'reading',\n",
       " 'images',\n",
       " 'bike',\n",
       " 'duke',\n",
       " 'bus',\n",
       " 'physics',\n",
       " 'thus',\n",
       " 'contact',\n",
       " 'ti',\n",
       " 'lines',\n",
       " 'monitor',\n",
       " 'easy',\n",
       " '\\nthat',\n",
       " 'hear',\n",
       " 'ee',\n",
       " 'uci',\n",
       " 'output',\n",
       " 'users',\n",
       " 'csd',\n",
       " 'test',\n",
       " 'manager',\n",
       " 'microsoft',\n",
       " 'discussion',\n",
       " 'anti',\n",
       " 'players',\n",
       " 'peace',\n",
       " 'convex',\n",
       " 'four',\n",
       " 'except',\n",
       " 'personal',\n",
       " 'experience',\n",
       " 'nntp',\n",
       " 'third',\n",
       " 'posted',\n",
       " 'needs',\n",
       " 'turn',\n",
       " 'interesting',\n",
       " 'brian',\n",
       " 'study',\n",
       " 'vms',\n",
       " 'keys',\n",
       " 'week',\n",
       " 'hit',\n",
       " 'exist',\n",
       " 'lost',\n",
       " 'answers',\n",
       " 'model',\n",
       " 'short',\n",
       " 'usually',\n",
       " 'happen',\n",
       " 'uses',\n",
       " 'add',\n",
       " 'bitnet',\n",
       " 'ufl',\n",
       " 'effect',\n",
       " 'kill',\n",
       " 'request',\n",
       " 'motif',\n",
       " 'special',\n",
       " 'numbers',\n",
       " 'optilink',\n",
       " 'months',\n",
       " 'application',\n",
       " 'front',\n",
       " 'pipex',\n",
       " 'error',\n",
       " 'particular',\n",
       " 'series',\n",
       " 'peter',\n",
       " 'bob',\n",
       " 'baseball',\n",
       " 'therefore',\n",
       " 'advance',\n",
       " 'jpeg',\n",
       " 'food',\n",
       " 'media',\n",
       " 'due',\n",
       " 'accept',\n",
       " 'air',\n",
       " 'bbs',\n",
       " 'mode',\n",
       " 'canada',\n",
       " 'season',\n",
       " 'horus',\n",
       " 'ones',\n",
       " 'face',\n",
       " 'uchicago',\n",
       " 'land',\n",
       " '1993apr5',\n",
       " 'sell',\n",
       " 'longer',\n",
       " 'expect',\n",
       " 'gas',\n",
       " 'astro',\n",
       " 'move',\n",
       " 'cd',\n",
       " 'greek',\n",
       " 'navy',\n",
       " 'drivers',\n",
       " '1993apr17',\n",
       " '\\n\\nyou',\n",
       " 'army',\n",
       " 'young',\n",
       " 'runs',\n",
       " 'job',\n",
       " 'newsserver',\n",
       " 'medical',\n",
       " 'mail\\nfrom',\n",
       " 'members',\n",
       " 'points',\n",
       " 'gets',\n",
       " 'uknet',\n",
       " 'future',\n",
       " 'speak',\n",
       " 'gap',\n",
       " 'jhu',\n",
       " 'sources',\n",
       " 'plus',\n",
       " '\\t\\t',\n",
       " '1993apr6',\n",
       " 'policy',\n",
       " 'insurance',\n",
       " 'according',\n",
       " 'tv',\n",
       " 'worth',\n",
       " 'taking',\n",
       " 'shall',\n",
       " 'office',\n",
       " 'values',\n",
       " 'built',\n",
       " '0d',\n",
       " 'save',\n",
       " '1993apr26',\n",
       " 'cars',\n",
       " '90',\n",
       " 'texas',\n",
       " 'thomas',\n",
       " 'mu',\n",
       " 'kent',\n",
       " 'process',\n",
       " 'build',\n",
       " 'i\\n',\n",
       " 'ux1',\n",
       " 'united',\n",
       " 'assume',\n",
       " '1993apr14',\n",
       " 'considered',\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the final feature_set\n",
    "feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the input dataset x\n",
    "def build_dataset(files,roots,feature_set):\n",
    "    X = np.zeros((19997,1500))\n",
    "    k=0\n",
    "    count = 0\n",
    "    #clean_words_data = []\n",
    "    for f in files:\n",
    "        k=k+1\n",
    "        for i in f:\n",
    "            file_object=open(roots[k] + \"/\" + i,\"r\")\n",
    "            file_data = file_object.read()\n",
    "            words_data = re.split(\"[\" + string.punctuation + \" \" + \"]\", file_data.lower())# splitting the document in word\n",
    "            clean_words_data = clean_data(words_data)\n",
    "            for w in clean_words_data:\n",
    "                if w in feature_set:\n",
    "                    i = feature_set.index(w)\n",
    "                    X[count][i] += 1\n",
    "            count = count + 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = build_dataset(files,roots,feature_set) # Here X and Y above is the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sklearn Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(x,y,random_state=0)\n",
    "# x and Y dataset is splitted into x_train and x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 45.,   9.,   1., ...,   0.,   0.,   0.],\n",
       "       [ 35.,   7.,   0., ...,   0.,   0.,   0.],\n",
       "       [ 91.,  10.,   2., ...,   0.,   0.,   0.],\n",
       "       ...,\n",
       "       [ 58.,   2.,   2., ...,   0.,   0.,   0.],\n",
       "       [259.,  16.,   5., ...,   0.,   0.,   0.],\n",
       "       [140.,   3.,   3., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.81      0.72      0.76       233\n",
      "           comp.graphics       0.64      0.58      0.61       253\n",
      " comp.os.ms-windows.misc       0.57      0.03      0.06       249\n",
      "comp.sys.ibm.pc.hardware       0.48      0.76      0.59       240\n",
      "   comp.sys.mac.hardware       0.43      0.82      0.57       236\n",
      "          comp.windows.x       0.66      0.60      0.63       240\n",
      "            misc.forsale       0.78      0.71      0.74       261\n",
      "               rec.autos       0.69      0.74      0.71       269\n",
      "         rec.motorcycles       0.78      0.87      0.82       284\n",
      "      rec.sport.baseball       0.63      0.82      0.71       248\n",
      "        rec.sport.hockey       0.60      0.45      0.52       231\n",
      "               sci.crypt       0.91      0.93      0.92       233\n",
      "         sci.electronics       0.73      0.72      0.72       244\n",
      "                 sci.med       0.76      0.65      0.70       256\n",
      "               sci.space       0.87      0.82      0.85       246\n",
      "  soc.religion.christian       0.97      1.00      0.98       252\n",
      "      talk.politics.guns       0.78      0.88      0.83       249\n",
      "   talk.politics.mideast       0.91      0.79      0.84       281\n",
      "      talk.politics.misc       0.80      0.64      0.71       259\n",
      "      talk.religion.misc       0.67      0.67      0.67       236\n",
      "\n",
      "             avg / total       0.73      0.71      0.70      5000\n",
      "\n",
      "[[168   0   0   1   5   0   2   7   2   1   2   2   0   5   0   0   1   0\n",
      "    0  37]\n",
      " [  0 146   1  18  45  18   6   2   0   0   3   1   8   3   2   0   0   0\n",
      "    0   0]\n",
      " [  0  33   8 129  24  46   4   1   0   0   0   2   1   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   1 183  42   2   7   0   1   0   1   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   4   1  18 194   4  11   0   0   0   2   0   1   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0  26   1  15  39 144   2   0   1   0   0   2   6   1   2   0   0   1\n",
      "    0   0]\n",
      " [  0   1   0   7  35   0 186  12   4   1   8   0   5   1   1   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0   2   6   0  10 200  27   0   6   0   6   4   4   1   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   2   1   3  18 246   3   4   0   2   2   0   1   1   0\n",
      "    1   0]\n",
      " [  1   0   0   0   1   0   1   8   6 203  26   0   0   1   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   1   0   2   0   0   7   7 103 105   0   1   0   4   0   1   0\n",
      "    0   0]\n",
      " [  1   3   1   0   1   1   0   0   2   0   1 217   3   1   0   0   2   0\n",
      "    0   0]\n",
      " [  0   4   0   8  20   0   4   8   3   0   7   2 175   7   5   0   0   1\n",
      "    0   0]\n",
      " [  1   6   0   2  17   0   0  16   9   8   0   3  18 167   6   0   1   1\n",
      "    0   1]\n",
      " [  0   1   0   1   4   2   2   8   4   1   0   2  13   4 202   0   1   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0   0   0 251   0   0\n",
      "    0   0]\n",
      " [  1   0   0   1   0   0   0   1   3   0   4   3   0   2   1   0 218   1\n",
      "   10   4]\n",
      " [  2   0   0   0   8   0   0   3   2   0   2   1   0   8   2   0   7 222\n",
      "   15   9]\n",
      " [  0   0   0   0   2   0   1   0   0   1   1   3   0   5   2   2  34  15\n",
      "  167  26]\n",
      " [ 34   0   0   0   0   0   0   0   0   0   1   0   1   6   0   5  11   4\n",
      "   16 158]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(classification_report(Y_test,Y_pred))\n",
    "print(confusion_matrix(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, Y_train):\n",
    "    result = {}\n",
    "    class_values = set(Y_train)\n",
    "    for current_class in class_values:\n",
    "        result[current_class] = {}\n",
    "        result[\"total_data\"] = len(Y_train)# total data in the result\n",
    "        current_class_rows = (Y_train == current_class)\n",
    "        X_train_current = X_train[current_class_rows]# all the training dataset containing the single class\n",
    "        Y_train_current = Y_train[current_class_rows]# output data corresponding to the particular class\n",
    "        num_features = X_train.shape[1]\n",
    "        result[current_class][\"total_count_1\"] = len(Y_train_current)\n",
    "        result[current_class][\"total_count_2\"] = X_train_current[:,:].sum() # total no of words in the single class\n",
    "        for j in range(0, num_features):\n",
    "            result[current_class][j] = X_train_current[:,j].sum() \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(dictionary,x,current_class):\n",
    "    output = (np.log(dictionary[current_class][\"total_count_1\"]) - np.log(dictionary[\"total_data\"]))\n",
    "    num_features = len(dictionary[current_class].keys())-2;\n",
    "    for j in range(0, num_features):\n",
    "        xj = x[j]\n",
    "        if(xj == 0): # if feature value is 0 we are not going to take that's probability\n",
    "            continue\n",
    "        # finding the probability of particular feature in the class\n",
    "        count_current_class_with_value_xj = dictionary[current_class][j] + 1\n",
    "        # applying the laplace correction on each feature\n",
    "        count_current_class = dictionary[current_class][\"total_count_2\"] + (len(dictionary[current_class].keys())-2)\n",
    "        current_xj_probablity = np.log(count_current_class_with_value_xj) - np.log(count_current_class)\n",
    "        output = output + current_xj_probablity\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for predicting the single point \n",
    "def predictSinglePoint(dictionary,x):\n",
    "    classes = dictionary.keys()\n",
    "    best_p = -1000000000\n",
    "    best_class = -1\n",
    "    first_run = True\n",
    "    for current_class in classes:\n",
    "        if(current_class == \"total_data\"):\n",
    "            continue\n",
    "        p_current_class = probability(dictionary,x, current_class)\n",
    "        if (first_run or p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False  # this is for ensuring that it run for the first time\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for predicting the output\n",
    "def predict(dictionary, X_test):\n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        x_class = predictSinglePoint(dictionary,x)\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = predict(dictionary,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.75      0.80      0.78       233\n",
      "           comp.graphics       0.65      0.34      0.44       253\n",
      " comp.os.ms-windows.misc       0.75      0.01      0.02       249\n",
      "comp.sys.ibm.pc.hardware       0.47      0.60      0.52       240\n",
      "   comp.sys.mac.hardware       0.25      0.99      0.40       236\n",
      "          comp.windows.x       0.97      0.15      0.25       240\n",
      "            misc.forsale       0.95      0.46      0.62       261\n",
      "               rec.autos       0.60      0.89      0.71       269\n",
      "         rec.motorcycles       0.97      0.69      0.81       284\n",
      "      rec.sport.baseball       0.51      0.98      0.67       248\n",
      "        rec.sport.hockey       1.00      0.06      0.11       231\n",
      "               sci.crypt       0.65      0.98      0.78       233\n",
      "         sci.electronics       0.73      0.78      0.76       244\n",
      "                 sci.med       1.00      0.34      0.50       256\n",
      "               sci.space       0.95      0.78      0.85       246\n",
      "  soc.religion.christian       0.97      1.00      0.98       252\n",
      "      talk.politics.guns       0.84      0.78      0.81       249\n",
      "   talk.politics.mideast       0.93      0.78      0.84       281\n",
      "      talk.politics.misc       0.68      0.72      0.70       259\n",
      "      talk.religion.misc       0.65      0.65      0.65       236\n",
      "\n",
      "             avg / total       0.77      0.64      0.62      5000\n",
      "\n",
      "[[186   0   0   0   4   0   0   6   0   2   0   5   1   0   0   0   0   0\n",
      "    0  29]\n",
      " [  1  85   0  11 129   0   0   0   0   0   0  20   6   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0  11   3 127  95   1   0   0   0   0   0   8   2   0   0   0   0   0\n",
      "    1   1]\n",
      " [  0   0   0 143  97   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   1 233   0   1   0   0   0   0   1   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  32   1  16 135  35   1   0   0   0   0  17   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   8 103   0 121  22   0   0   0   1   5   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0  13   0   4 240   4   4   0   1   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   0   0   0   6   0   0  71 197   3   0   1   1   0   0   0   2   0\n",
      "    2   0]\n",
      " [  0   0   0   0   1   0   0   4   0 243   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   2   0   0   4   2 208  13   1   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   3   0   0   0   0   0   0 228   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   1  37   0   0   8   0   1   0   6 190   0   1   0   0   0\n",
      "    0   0]\n",
      " [  8   2   0   0  34   0   0  35   0   9   0  27  32  86   5   0   0   1\n",
      "    5  12]\n",
      " [  0   0   0   0  10   0   0   7   0   4   0  17  13   0 191   0   0   0\n",
      "    4   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 252   0   0\n",
      "    0   0]\n",
      " [  1   0   0   0   3   0   0   2   0   0   0   9   0   0   0   0 194   0\n",
      "   33   7]\n",
      " [  5   0   0   0  10   0   0   4   0   1   0   7   0   0   1   1   5 218\n",
      "   22   7]\n",
      " [  1   0   0   0   3   0   0   0   0   1   0   3   0   0   1   0  21  14\n",
      "  187  28]\n",
      " [ 44   0   0   0   1   0   0   0   0   0   0   1   0   0   0   7   9   2\n",
      "   19 153]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(Y_test,Y_pred))\n",
    "print(confusion_matrix(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hence by looking at classification report WE can compare both inbuilt sklearn multionomialNB and the own implementation\n",
    "# of multinomial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
